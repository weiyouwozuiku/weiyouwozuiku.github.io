---
title: 大数据处理方案
author: 不二
img: >-
  https://cdn.jsdelivr.net/gh/weiyouwozuiku/weiyouwozuiku.github.io@src/source/_posts/PageImg/算法/大数据处理方案.jpeg
mathjax: true
date: 2022-01-24 17:25:55
tags: 去重
categories: 算法
---

## 背景

所谓的海量数据是指海量数据的存储、处理和操作。正是因为数据量太大，所以导致要么无法在短时间迅速完成，要么无法一次性载入内存。

数据去重（data deduplication）是大数据领域司空见惯的问题了。除了统计UV等传统用法之外，去重的意义更在于消除不可靠数据源产生的脏数据——即重复上报数据或重复投递数据的影响，使计算产生的结果更加准确。

## 参考文献

1. 博客园，[大数据去重（data deduplication）方案](https://www.cnblogs.com/luxiaoxun/p/14392375.html)，2021
1. GitBook，[第六章 海量数据处理](https://wizardforcel.gitbooks.io/the-art-of-programming-by-july/content/ch6.html)
